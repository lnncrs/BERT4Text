{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning de modelos BERT para classificação de laudos médicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é o BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional Encoder Representations from Transformers ou BERT é um Large Language Model (LLM) desenvolvido pela Google AI Language e publicado como [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2018)](https://arxiv.org/abs/1810.04805).\n",
    "\n",
    "O modelo original foi lançado logo após o lançamento público do primeiro membro da familia Generative Pre-trained Transformer (GPT) pela Open AI publicado como [Improving Language Understanding by Generative Pre-Training (Radfors et al., 2018)](https://openai.com/index/language-unsupervised/)\n",
    "\n",
    "As duas implementações são baseadas na arquitetura Transformer, introduzida em [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "<mark>Enquanto modelos LLM GPT focam em Natural Language Generation (NLG), modelos LLM BERT são voltados a Natural Language Understanding (NLU).</mark>\n",
    "\n",
    "Ambas as arquiteturas **decoder-only** do GPT e **encoder-only** do BERT são capazes de produzir modelos formidáveis, mas as tarefas que eles são capazes de realizar têm diferenças conceituais e de implementação importantes.\n",
    "\n",
    "O desenho de uma solução bem sucedida baseada na arquitetura Transformer depende do entendimento claro dessa diferença sutil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que é a arquitetura Transformer, GPT e BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo as três arquiteturas discutidas até aqui esquematizadas\n",
    "\n",
    "![Transformer, GPT e BERT](../assets/1_Qww2aaIdqrWVeNmo3AS0ZQ.png)\n",
    "\n",
    "Como visto em [A Complete Guide to BERT with Code](https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contexto Bidirecional x Unidirecional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O conceito de bidirecional que é ums das diferenças chave do BERT está ligado ao fato de que cada palavra na sequencia pode receber contexto de palavras anteriores e posteriores, em outros termos, o mecanismo de atenção da arquitetura pode se concentrar nos tokens à direita e à esquerda.\n",
    "\n",
    "Abaixo uma ilustração dos dois mecanismos de atenção comparados\n",
    "\n",
    "![Mecanismos de atenção](../assets/1_otV3y8jKM_zSxA-0YiLi1w.png)\n",
    "\n",
    "Como visto em [A Complete Guide to BERT with Code](https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "O BERT modelo foi o primeiro derivado da arquitetura Transformer baseado codificadores, mas ele utilizou diversos conceitos usados antes na arquitetura GPT de decodificadores, um destes conceitos muito importante para a terefa proposta aqui é a do **pré-treinamento de modelos**.\n",
    "\n",
    "O pré-treinamento envolve treinar um modelo para adquirir um entendimento amplo da linguagem, com o objetivo de criar um modelo fundamental independente de tarefas. Nos diagramas apresentados, o modelo fundamental é composto pelos componentes abaixo da penultima camada de baixo para cima.\n",
    "\n",
    "Uma vez treinado, cópias desse modelo fundamental podem ser ajustadas para resolver tarefas específicas. O ajuste fino (fine-tuning) envolve treinar apenas a camada linear: uma pequena rede neural feedforward, frequentemente chamada de \"classification head\" ou simplesmente \"head\". Os pesos e vieses no restante do modelo permanecem inalterados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variantes mais comuns do BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[RoBERTa (Liu et al., 2019)](https://arxiv.org/abs/1907.11692)**\n",
    "\n",
    "Remove NSP, aumenta o tamanho do batch e utiliza mais dados. Treinamento ajustado exclusivamente em MLM.\n",
    "\n",
    "**[DistilBERT (Sanh et al., 2019)](https://arxiv.org/abs/1910.01108)**\n",
    "\n",
    "Usa knowledge distillation para reduzir o tamanho, mantendo 97% da performance. Menor custo de inferência.\n",
    "\n",
    "**[BioBERT (Lee et al., 2020)](https://arxiv.org/abs/1901.08746)**\n",
    "\n",
    "Pré-treinado em textos biomédicos, como PubMed e PMC. Otimizado para tarefas de NER e extração de informações biomédicas.\n",
    "\n",
    "**[ClinicalBERT (Huang et al., 2019)](https://arxiv.org/abs/1904.05342)**\n",
    "\n",
    "Pré-treinado em registros médicos eletrônicos, com foco em tarefas clínicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura adicional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma explicação detalhada da arquitetura está além do escopo deste material, mas é altamente recomendada  aleitura do artigo técnico [A Complete Guide to BERT with Code](https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados utilizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O conjunto de dados utilizado como base foi o [Breast Cancer Wisconsin (Diagnostic)](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic) ele é amplamente utilizado para análise e classificação de câncer de mama. Ele contém medições computacionais de características extraídas de imagens de massas celulares em exames de mamografia. As características foram calculadas a partir de imagens digitalizadas de aspirações por agulha fina (FNA), sendo usadas para prever se uma massa é maligna (câncer) ou benigna (não cancerígena). Estes dados foram disponibilizados no repositório [UCI Machine Learning](https://archive.ics.uci.edu/) e originalmente coletados pelo Dr. William H. Wolberg da University of Wisconsin Hospitals, Madison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A preparação dos dados está no notebook [PrepareData](PrepareData.ipynb).\n",
    "\n",
    "Os dados originais foram aumentados sinteticamente somente para fins da demonstração do passo a passo do finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codigo completo temporário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    get_linear_schedule_with_warmup)\n",
    "\n",
    "def preprocess_dataset(path):\n",
    "    \"\"\" Remove unnecessary characters and encode the sentiment labels.\n",
    "\n",
    "    The type of preprocessing required changes based on the dataset. For the\n",
    "    IMDb dataset, the review texts contains HTML break tags (<br/>) leftover\n",
    "    from the scraping process, and some unnecessary whitespace, which are\n",
    "    removed. Finally, encode the sentiment labels as 0 for \"negative\" and 1 for\n",
    "    \"positive\". This method assumes the dataset file contains the headers\n",
    "    \"review\" and \"sentiment\".\n",
    "\n",
    "    Parameters:\n",
    "        path (str): A path to a dataset file containing the sentiment analysis\n",
    "            dataset. The structure of the file should be as follows: one column\n",
    "            called \"review\" containing the review text, and one column called\n",
    "            \"sentiment\" containing the ground truth label. The label options\n",
    "            should be \"negative\" and \"positive\".\n",
    "\n",
    "    Returns:\n",
    "        df_dataset (pd.DataFrame): A DataFrame containing the raw data\n",
    "            loaded from the self.dataset path. In addition to the expected\n",
    "            \"review\" and \"sentiment\" columns, are:\n",
    "\n",
    "            > laudo_cleaned - a copy of the \"review\" column with the HTML\n",
    "                break tags and unnecessary whitespace removed\n",
    "\n",
    "            > diagnosis_encoded - a copy of the \"sentiment\" column with the\n",
    "                \"negative\" values mapped to 0 and \"positive\" values mapped\n",
    "                to 1\n",
    "    \"\"\"\n",
    "    df_dataset = pd.read_parquet(path)\n",
    "\n",
    "    # df_dataset['laudo_cleaned'] = df_dataset['review'].\\\n",
    "    #     apply(lambda x: x.replace('<br />', ''))\n",
    "\n",
    "    # df_dataset['laudo_cleaned'] = df_dataset['laudo_cleaned'].\\\n",
    "    #     replace('\\s+', ' ', regex=True)\n",
    "\n",
    "    df_dataset['laudo_cleaned'] = df_dataset['laudo']\n",
    "\n",
    "    df_dataset['diagnosis_encoded'] = df_dataset['diagnosis'].\\\n",
    "        apply(lambda x: 0 if x == 'M' else 1)\n",
    "\n",
    "    return df_dataset\n",
    "\n",
    "class FineTuningPipeline:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset,\n",
    "            tokenizer,\n",
    "            model,\n",
    "            optimizer,\n",
    "            loss_function = nn.CrossEntropyLoss(),\n",
    "            val_size = 0.1,\n",
    "            epochs = 4,\n",
    "            seed = 42):\n",
    "\n",
    "        self.df_dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_function\n",
    "        self.val_size = val_size\n",
    "        self.epochs = epochs\n",
    "        self.seed = seed\n",
    "\n",
    "        # Check if GPU is available for faster training time\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda:0')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "\n",
    "        # Perform fine-tuning\n",
    "        self.model.to(self.device)\n",
    "        self.set_seeds()\n",
    "        self.token_ids, self.attention_masks = self.tokenize_dataset()\n",
    "        self.train_dataloader, self.val_dataloader = self.create_dataloaders()\n",
    "        self.scheduler = self.create_scheduler()\n",
    "        self.fine_tune()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\" Tokenize input text and return the token IDs and attention mask.\n",
    "\n",
    "        Tokenize an input string, setting a maximum length of 512 tokens.\n",
    "        Sequences with more than 512 tokens will be truncated to this limit,\n",
    "        and sequences with less than 512 tokens will be supplemented with [PAD]\n",
    "        tokens to bring them up to this limit. The datatype of the returned\n",
    "        tensors will be the PyTorch tensor format. These return values are\n",
    "        tensors of size 1 x max_length where max_length is the maximum number\n",
    "        of tokens per input sequence (512 for BERT).\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            token_ids (torch.Tensor): A tensor of token IDs for each token in\n",
    "                the input sequence.\n",
    "\n",
    "            attention_mask (torch.Tensor): A tensor of 1s and 0s where a 1\n",
    "                indicates a token can be attended to during the attention\n",
    "                process, and a 0 indicates a token should be ignored. This is\n",
    "                used to prevent BERT from attending to [PAD] tokens during its\n",
    "                training/inference.\n",
    "        \"\"\"\n",
    "        batch_encoder = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length = 512,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_tensors = 'pt')\n",
    "\n",
    "        token_ids = batch_encoder['input_ids']\n",
    "        attention_mask = batch_encoder['attention_mask']\n",
    "\n",
    "        return token_ids, attention_mask\n",
    "\n",
    "    def tokenize_dataset(self):\n",
    "        \"\"\" Apply the self.tokenize method to the fine-tuning dataset.\n",
    "\n",
    "        Tokenize and return the input sequence for each row in the fine-tuning\n",
    "        dataset given by self.dataset. The return values are tensors of size\n",
    "        len_dataset x max_length where len_dataset is the number of rows in the\n",
    "        fine-tuning dataset and max_length is the maximum number of tokens per\n",
    "        input sequence (512 for BERT).\n",
    "\n",
    "        Parameters:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            token_ids (torch.Tensor): A tensor of tensors containing token IDs\n",
    "            for each token in the input sequence.\n",
    "\n",
    "            attention_masks (torch.Tensor): A tensor of tensors containing the\n",
    "                attention masks for each sequence in the fine-tuning dataset.\n",
    "        \"\"\"\n",
    "        token_ids = []\n",
    "        attention_masks = []\n",
    "\n",
    "        for review in self.df_dataset['laudo_cleaned']:\n",
    "            tokens, masks = self.tokenize(review)\n",
    "            token_ids.append(tokens)\n",
    "            attention_masks.append(masks)\n",
    "\n",
    "        token_ids = torch.cat(token_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "        return token_ids, attention_masks\n",
    "\n",
    "    def create_dataloaders(self):\n",
    "        \"\"\" Create dataloaders for the train and validation set.\n",
    "\n",
    "        Split the tokenized dataset into train and validation sets according to\n",
    "        the self.val_size value. For example, if self.val_size is set to 0.1,\n",
    "        90% of the data will be used to form the train set, and 10% for the\n",
    "        validation set. Convert the \"diagnosis_encoded\" column (labels for each\n",
    "        row) to PyTorch tensors to be used in the dataloaders.\n",
    "\n",
    "        Parameters:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            train_dataloader (torch.utils.data.dataloader.DataLoader): A\n",
    "                dataloader of the train data, including the token IDs,\n",
    "                attention masks, and sentiment labels.\n",
    "\n",
    "            val_dataloader (torch.utils.data.dataloader.DataLoader): A\n",
    "                dataloader of the validation data, including the token IDs,\n",
    "                attention masks, and sentiment labels.\n",
    "\n",
    "        \"\"\"\n",
    "        train_ids, val_ids = train_test_split(\n",
    "                        self.token_ids,\n",
    "                        test_size=self.val_size,\n",
    "                        shuffle=False)\n",
    "\n",
    "        train_masks, val_masks = train_test_split(\n",
    "                                    self.attention_masks,\n",
    "                                    test_size=self.val_size,\n",
    "                                    shuffle=False)\n",
    "\n",
    "        labels = torch.tensor(self.df_dataset['diagnosis_encoded'].values)\n",
    "        train_labels, val_labels = train_test_split(\n",
    "                                        labels,\n",
    "                                        test_size=self.val_size,\n",
    "                                        shuffle=False)\n",
    "\n",
    "        train_data = TensorDataset(train_ids, train_masks, train_labels)\n",
    "        train_dataloader = DataLoader(train_data, shuffle=True, batch_size=16)\n",
    "        val_data = TensorDataset(val_ids, val_masks, val_labels)\n",
    "        val_dataloader = DataLoader(val_data, batch_size=16)\n",
    "\n",
    "        return train_dataloader, val_dataloader\n",
    "\n",
    "    def create_scheduler(self):\n",
    "        \"\"\" Create a linear scheduler for the learning rate.\n",
    "\n",
    "        Create a scheduler with a learning rate that increases linearly from 0\n",
    "        to a maximum value (called the warmup period), then decreases linearly\n",
    "        to 0 again. num_warmup_steps is set to 0 here based on an example from\n",
    "        Hugging Face:\n",
    "\n",
    "        https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2\n",
    "        d008813037968a9e58/examples/run_glue.py#L308\n",
    "\n",
    "        Read more about schedulers here:\n",
    "\n",
    "        https://huggingface.co/docs/transformers/main_classes/optimizer_\n",
    "        schedules#transformers.get_linear_schedule_with_warmup\n",
    "        \"\"\"\n",
    "        num_training_steps = self.epochs * len(self.train_dataloader)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=num_training_steps)\n",
    "\n",
    "        return scheduler\n",
    "\n",
    "    def set_seeds(self):\n",
    "        \"\"\" Set the random seeds so that results are reproduceable.\n",
    "\n",
    "        Parameters:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "        torch.cuda.manual_seed_all(self.seed)\n",
    "\n",
    "    def fine_tune(self):\n",
    "        \"\"\"Train the classification head on the BERT model.\n",
    "\n",
    "        Fine-tune the model by training the classification head (linear layer)\n",
    "        sitting on top of the BERT model. The model trained on the data in the\n",
    "        self.train_dataloader, and validated at the end of each epoch on the\n",
    "        data in the self.val_dataloader. The series of steps are described\n",
    "        below:\n",
    "\n",
    "        Training:\n",
    "\n",
    "        > Create a dictionary to store the average training loss and average\n",
    "          validation loss for each epoch.\n",
    "        > Store the time at the start of training, this is used to calculate\n",
    "          the time taken for the entire training process.\n",
    "        > Begin a loop to train the model for each epoch in self.epochs.\n",
    "\n",
    "        For each epoch:\n",
    "\n",
    "        > Switch the model to train mode. This will cause the model to behave\n",
    "          differently than when in evaluation mode (e.g. the batchnorm and\n",
    "          dropout layers are activated in train mode, but disabled in\n",
    "          evaluation mode).\n",
    "        > Set the training loss to 0 for the start of the epoch. This is used\n",
    "          to track the loss of the model on the training data over subsequent\n",
    "          epochs. The loss should decrease with each epoch if training is\n",
    "          successful.\n",
    "        > Store the time at the start of the epoch, this is used to calculate\n",
    "          the time taken for the epoch to be completed.\n",
    "        > As per the BERT authors' recommendations, the training data for each\n",
    "          epoch is split into batches. Loop through the training process for\n",
    "          each batch.\n",
    "\n",
    "        For each batch:\n",
    "\n",
    "        > Move the token IDs, attention masks, and labels to the GPU if\n",
    "          available for faster processing, otherwise these will be kept on the\n",
    "          CPU.\n",
    "        > Invoke the zero_grad method to reset the calculated gradients from\n",
    "          the previous iteration of this loop.\n",
    "        > Pass the batch to the model to calculate the logits (predictions\n",
    "          based on the current classifier weights and biases) as well as the\n",
    "          loss.\n",
    "        > Increment the total loss for the epoch. The loss is returned from the\n",
    "          model as a PyTorch tensor so extract the float value using the item\n",
    "          method.\n",
    "        > Perform a backward pass of the model and propagate the loss through\n",
    "          the classifier head. This will allow the model to determine what\n",
    "          adjustments to make to the weights and biases to improve its\n",
    "          performance on the batch.\n",
    "        > Clip the gradients to be no larger than 1.0 so the model does not\n",
    "          suffer from the exploding gradients problem.\n",
    "        > Call the optimizer to take a step in the direction of the error\n",
    "          surface as determined by the backward pass.\n",
    "\n",
    "        After training on each batch:\n",
    "\n",
    "        > Calculate the average loss and time taken for training on the epoch.\n",
    "\n",
    "        Validation step for the epoch:\n",
    "\n",
    "        > Switch the model to evaluation mode.\n",
    "        > Set the validation loss to 0. This is used to track the loss of the\n",
    "          model on the validation data over subsequent epochs. The loss should\n",
    "          decrease with each epoch if training was successful.\n",
    "        > Store the time at the start of the validation, this is used to\n",
    "          calculate the time taken for the validation for this epoch to be\n",
    "          completed.\n",
    "        > Split the validation data into batches.\n",
    "\n",
    "        For each batch:\n",
    "\n",
    "        > Move the token IDs, attention masks, and labels to the GPU if\n",
    "          available for faster processing, otherwise these will be kept on the\n",
    "          CPU.\n",
    "        > Invoke the no_grad method to instruct the model not to calculate the\n",
    "          gradients since we wil not be performing any optimization steps here,\n",
    "          only inference.\n",
    "        > Pass the batch to the model to calculate the logits (predictions\n",
    "          based on the current classifier weights and biases) as well as the\n",
    "          loss.\n",
    "        > Extract the logits and labels from the model and move them to the CPU\n",
    "          (if they are not already there).\n",
    "        > Increment the loss and calculate the accuracy based on the true\n",
    "          labels in the validation dataloader.\n",
    "        > Calculate the average loss and accuracy, and add these to the loss\n",
    "          dictionary.\n",
    "        \"\"\"\n",
    "\n",
    "        loss_dict = {\n",
    "            'epoch': [i+1 for i in range(self.epochs)],\n",
    "            'average training loss': [],\n",
    "            'average validation loss': []\n",
    "        }\n",
    "\n",
    "        t0_train = datetime.now()\n",
    "\n",
    "        for epoch in range(0, self.epochs):\n",
    "\n",
    "            # Train step\n",
    "            self.model.train()\n",
    "            training_loss = 0\n",
    "            t0_epoch = datetime.now()\n",
    "\n",
    "            print(f'{\"-\"*20} Epoch {epoch+1} {\"-\"*20}')\n",
    "            print('\\nTraining:\\n---------')\n",
    "            print(f'Start Time:       {t0_epoch}')\n",
    "\n",
    "            for batch in self.train_dataloader:\n",
    "\n",
    "                batch_token_ids = batch[0].to(self.device)\n",
    "                batch_attention_mask = batch[1].to(self.device)\n",
    "                batch_labels = batch[2].to(self.device)\n",
    "\n",
    "                self.model.zero_grad()\n",
    "\n",
    "                loss, logits = self.model(\n",
    "                    batch_token_ids,\n",
    "                    token_type_ids = None,\n",
    "                    attention_mask=batch_attention_mask,\n",
    "                    labels=batch_labels,\n",
    "                    return_dict=False)\n",
    "\n",
    "                training_loss += loss.item()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "\n",
    "            average_train_loss = training_loss / len(self.train_dataloader)\n",
    "            time_epoch = datetime.now() - t0_epoch\n",
    "\n",
    "            print(f'Average Loss:     {average_train_loss}')\n",
    "            print(f'Time Taken:       {time_epoch}')\n",
    "\n",
    "            # Validation step\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            val_accuracy = 0\n",
    "            t0_val = datetime.now()\n",
    "\n",
    "            print('\\nValidation:\\n---------')\n",
    "            print(f'Start Time:       {t0_val}')\n",
    "\n",
    "            for batch in self.val_dataloader:\n",
    "\n",
    "                batch_token_ids = batch[0].to(self.device)\n",
    "                batch_attention_mask = batch[1].to(self.device)\n",
    "                batch_labels = batch[2].to(self.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    (loss, logits) = self.model(\n",
    "                        batch_token_ids,\n",
    "                        attention_mask = batch_attention_mask,\n",
    "                        labels = batch_labels,\n",
    "                        token_type_ids = None,\n",
    "                        return_dict=False)\n",
    "\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = batch_labels.to('cpu').numpy()\n",
    "                val_loss += loss.item()\n",
    "                val_accuracy += self.calculate_accuracy(logits, label_ids)\n",
    "\n",
    "\n",
    "            average_val_accuracy = val_accuracy / len(self.val_dataloader)\n",
    "            average_val_loss = val_loss / len(self.val_dataloader)\n",
    "            time_val = datetime.now() - t0_val\n",
    "\n",
    "            print(f'Average Loss:     {average_val_loss}')\n",
    "            print(f'Average Accuracy: {average_val_accuracy}')\n",
    "            print(f'Time Taken:       {time_val}\\n')\n",
    "\n",
    "            loss_dict['average training loss'].append(average_train_loss)\n",
    "            loss_dict['average validation loss'].append(average_val_loss)\n",
    "\n",
    "        print(f'Total training time: {datetime.now()-t0_train}')\n",
    "\n",
    "    def calculate_accuracy(self, preds, labels):\n",
    "        \"\"\" Calculate the accuracy of model predictions against true labels.\n",
    "\n",
    "        Parameters:\n",
    "            preds (np.array): The predicted label from the model\n",
    "            labels (np.array): The true label\n",
    "\n",
    "        Returns:\n",
    "            accuracy (float): The accuracy as a percentage of the correct\n",
    "                predictions.\n",
    "        \"\"\"\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        accuracy = np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def predict(self, dataloader):\n",
    "        \"\"\"Return the predicted probabilities of each class for input text.\n",
    "        \n",
    "        Parameters:\n",
    "            dataloader (torch.utils.data.DataLoader): A DataLoader containing\n",
    "                the token IDs and attention masks for the text to perform\n",
    "                inference on.\n",
    "        \n",
    "        Returns:\n",
    "            probs (PyTorch.Tensor): A tensor containing the probability values\n",
    "                for each class as predicted by the model.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.eval()\n",
    "        all_logits = []\n",
    "\n",
    "        for batch in dataloader:\n",
    "\n",
    "            batch_token_ids, batch_attention_mask = tuple(t.to(self.device) \\\n",
    "                for t in batch)[:2]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(batch_token_ids, batch_attention_mask)\n",
    "\n",
    "            all_logits.append(logits)\n",
    "\n",
    "        all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "        probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/breast_cancer.parquet\"\n",
    "\n",
    "# Initialise parameters\n",
    "dataset = preprocess_dataset(data_path)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2)\n",
    "optimizer = AdamW(model.parameters())\n",
    "\n",
    "# Fine-tune model using class\n",
    "fine_tuned_model = FineTuningPipeline(\n",
    "    dataset = dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    val_size = 0.1,\n",
    "    epochs = 2,\n",
    "    seed = 42\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some predictions using the validation dataset\n",
    "fine_tuned_model.predict(fine_tuned_model.val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquisição do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo de uso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "abc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
