{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning de modelos BERT para classificação de laudos médicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é o BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional Encoder Representations from Transformers ou BERT é um Large Language Model (LLM) desenvolvido pela Google AI Language e publicado como [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2018)](https://arxiv.org/abs/1810.04805).\n",
    "\n",
    "O modelo original foi lançado logo após o lançamento público do primeiro membro da familia Generative Pre-trained Transformer (GPT) pela Open AI publicado como [Improving Language Understanding by Generative Pre-Training (Radfors et al., 2018)](https://openai.com/index/language-unsupervised/)\n",
    "\n",
    "As duas implementações são baseadas na arquitetura Transformer, introduzida em [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "<mark>Enquanto modelos LLM GPT focam em Natural Language Generation (NLG), modelos LLM BERT são voltados a Natural Language Understanding (NLU).</mark>\n",
    "\n",
    "Ambas as arquiteturas **decoder-only** do GPT e **encoder-only** do BERT são capazes de produzir modelos formidáveis, mas as tarefas que eles são capazes de realizar têm diferenças conceituais e de implementação importantes.\n",
    "\n",
    "O desenho de uma solução bem sucedida baseada na arquitetura Transformer depende do entendimento claro dessa diferença sutil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que é a arquitetura Transformer, GPT e BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo as três arquiteturas discutidas até aqui esquematizadas\n",
    "\n",
    "![Transformer, GPT e BERT](../assets/1_Qww2aaIdqrWVeNmo3AS0ZQ.png)\n",
    "\n",
    "Como visto em [A Complete Guide to BERT with Code](https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contexto Bidirecional x Unidirecional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O conceito de bidirecional que é ums das diferenças chave do BERT está ligado ao fato de que cada palavra na sequencia pode receber contexto de palavras anteriores e posteriores, em outros termos, o mecanismo de atenção da arquitetura pode se concentrar nos tokens à direita e à esquerda.\n",
    "\n",
    "Abaixo uma ilustração dos dois mecanismos de atenção comparados\n",
    "\n",
    "![Mecanismos de atenção](../assets/1_otV3y8jKM_zSxA-0YiLi1w.png)\n",
    "\n",
    "Como visto em [A Complete Guide to BERT with Code](https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "O BERT modelo foi o primeiro derivado da arquitetura Transformer baseado codificadores, mas ele utilizou diversos conceitos usados antes na arquitetura GPT de decodificadores, um destes conceitos muito importante para a terefa proposta aqui é a do **pré-treinamento de modelos**.\n",
    "\n",
    "O pré-treinamento envolve treinar um modelo para adquirir um entendimento amplo da linguagem, com o objetivo de criar um modelo fundamental independente de tarefas. Nos diagramas apresentados, o modelo fundamental é composto pelos componentes abaixo da penultima camada de baixo para cima.\n",
    "\n",
    "Uma vez treinado, cópias desse modelo fundamental podem ser ajustadas para resolver tarefas específicas. O ajuste fino (fine-tuning) envolve treinar apenas a camada linear: uma pequena rede neural feedforward, frequentemente chamada de \"classification head\" ou simplesmente \"head\". Os pesos e vieses no restante do modelo permanecem inalterados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variantes mais comuns do BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[RoBERTa (Liu et al., 2019)](https://arxiv.org/abs/1907.11692)**\n",
    "\n",
    "Remove NSP, aumenta o tamanho do batch e utiliza mais dados. Treinamento ajustado exclusivamente em MLM.\n",
    "\n",
    "**[DistilBERT (Sanh et al., 2019)](https://arxiv.org/abs/1910.01108)**\n",
    "\n",
    "Usa knowledge distillation para reduzir o tamanho, mantendo 97% da performance. Menor custo de inferência.\n",
    "\n",
    "**[BioBERT (Lee et al., 2020)](https://arxiv.org/abs/1901.08746)**\n",
    "\n",
    "Pré-treinado em textos biomédicos, como PubMed e PMC. Otimizado para tarefas de NER e extração de informações biomédicas.\n",
    "\n",
    "**[ClinicalBERT (Huang et al., 2019)](https://arxiv.org/abs/1904.05342)**\n",
    "\n",
    "Pré-treinado em registros médicos eletrônicos, com foco em tarefas clínicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura adicional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma explicação detalhada da arquitetura está além do escopo deste material, mas é altamente recomendada  aleitura do artigo técnico [A Complete Guide to BERT with Code](https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados utilizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O conjunto de dados utilizado como base foi o [Breast Cancer Wisconsin (Diagnostic)](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic) ele é amplamente utilizado para análise e classificação de câncer de mama. Ele contém medições computacionais de características extraídas de imagens de massas celulares em exames de mamografia. As características foram calculadas a partir de imagens digitalizadas de aspirações por agulha fina (FNA), sendo usadas para prever se uma massa é maligna (câncer) ou benigna (não cancerígena). Estes dados foram disponibilizados no repositório [UCI Machine Learning](https://archive.ics.uci.edu/) e originalmente coletados pelo Dr. William H. Wolberg da University of Wisconsin Hospitals, Madison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A preparação dos dados está no notebook [PrepareData](PrepareData.ipynb).\n",
    "\n",
    "Os dados originais foram aumentados sinteticamente somente para fins da demonstração do passo a passo do finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codigo completo temporário"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquisição do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo de uso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "abc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
